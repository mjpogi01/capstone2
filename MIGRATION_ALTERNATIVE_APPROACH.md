# Alternative Migration Approach

The connection pooler keeps terminating connections during large imports. Here are better alternatives:

## âœ… Recommended: Manual Import via Supabase SQL Editor (Chunks)

Since the pooler has limits, the most reliable way is to import in smaller chunks via Supabase Dashboard:

### Step 1: Split the Backup File

The backup file is 50MB with 56,801 lines. Split it into smaller chunks:

**Using PowerShell:**

```powershell
# Split into files of 5,000 lines each
$lines = Get-Content "backups\migration-backup-2025-11-20T18-53-25.sql"
$chunkSize = 5000
$chunkNumber = 1

for ($i = 0; $i -lt $lines.Count; $i += $chunkSize) {
    $chunk = $lines[$i..([Math]::Min($i + $chunkSize - 1, $lines.Count - 1))]
    $chunk | Out-File "backups\chunk-$chunkNumber.sql" -Encoding UTF8
    Write-Host "Created chunk $chunkNumber ($($chunk.Count) lines)"
    $chunkNumber++
}
```

This will create files like:
- `chunk-1.sql` (first 5,000 lines)
- `chunk-2.sql` (next 5,000 lines)
- etc.

### Step 2: Import Each Chunk via Supabase SQL Editor

1. Go to Supabase Dashboard â†’ Your New Project
2. Click **SQL Editor** â†’ **New Query**
3. Open `chunk-1.sql` in a text editor
4. Copy the contents (Ctrl+A, Ctrl+C)
5. Paste into SQL Editor
6. Click **Run** (or Ctrl+Enter)
7. Wait for it to complete
8. Repeat for `chunk-2.sql`, `chunk-3.sql`, etc.

**Tips:**
- Import chunks in order (1, 2, 3, ...)
- Wait for each chunk to finish before starting the next
- If a chunk fails, note which one and try again
- Smaller chunks (2,000-3,000 lines) are safer if you get errors

---

## âœ… Alternative: Use Supabase CLI

If you have Supabase CLI installed:

```bash
# Install Supabase CLI
npm install -g supabase

# Login
supabase login

# Link your new project
supabase link --project-ref kjqcswjljgavigyfzauj

# Import the backup
supabase db restore < backups/migration-backup-2025-11-20T18-53-25.sql
```

This uses Supabase's direct connection and handles large files better.

---

## âœ… Alternative: Import via psql (if you have PostgreSQL tools)

```powershell
# Set password
$env:PGPASSWORD="your-database-password"

# Import (this might still hit pooler limits, but worth trying)
psql "postgresql://postgres.kjqcswjljgavigyfzauj:password@aws-1-ap-southeast-1.pooler.supabase.com:5432/postgres" -f "backups\migration-backup-2025-11-20T18-53-25.sql"
```

---

## ðŸ“‹ Quick Script to Split Backup File

I can create a script to automatically split your backup file into manageable chunks. Would you like me to create that?

---

## ðŸ’¡ Why This Happens

Connection poolers have limits:
- **Transaction duration** (often 5-10 minutes max)
- **Idle timeout** (connections close after inactivity)
- **Connection limits** (too many operations = termination)

For 50MB files with 56,000+ statements, the pooler will likely terminate connections.

---

## ðŸŽ¯ Best Approach for Your Situation

**Given the connection terminations, I recommend:**

1. **Split the backup file** into chunks of 3,000-5,000 lines
2. **Import each chunk** via Supabase SQL Editor
3. **Verify data** after each chunk
4. **Continue** until all chunks are imported

This is more reliable than trying to import everything at once through the pooler.

Would you like me to create a script to split the backup file automatically?










